{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6120b5e",
   "metadata": {},
   "source": [
    "Inverted index search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4bbffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9379509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76000521",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"updated_data_vectorized.json\", \"r\") as outfile:\n",
    "    vectorized_data = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0938a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus_vocab.json\", \"r\") as outfile:\n",
    "    vocab_data = json.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af888274",
   "metadata": {},
   "source": [
    "Creating an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "025a683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pandemic', 0.03379752679269161), ('Epidemiology of HIV/AIDS', 0.0029201594086959858), ('Antonine Plague', 0.006486766320080395), ('Cholera', 0.0017927560926804468), ('COVID-19 pandemic', 0.009881004510820137), ('Crimson Contagion', 0.016553890673971398), ('HIV/AIDS', 0.0016793802133014462), ('Pandemic prevention', 0.08261617660435726), ('Pandemic Severity Assessment Framework', 0.03147282918261229), ('Pandemic severity index', 0.027411818965501027), ('Plague of Cyprian', 0.015736414591306144), ('PREDICT (USAID)', 0.02023253304596504), ('1929â€“1930 psittacosis pandemic', 0.006012498027810367), ('Science diplomacy and pandemics', 0.004520033978353892), ('Spanish flu', 0.019024620625310414), ('Swine influenza', 0.006744177681988347), ('Unified Victim Identification System', 0.0028515650601695698)]\n"
     ]
    }
   ],
   "source": [
    "# creating an inverted index for every word in vocab_data based upon the vocab_data as well as the vectorized_data\n",
    "inverted_index = {}\n",
    "\n",
    "for i, word in enumerate(vocab_data):\n",
    "    inverted_index[word] = []\n",
    "\n",
    "    for doc in vectorized_data:\n",
    "        if doc[\"tf_idf\"][i] != 0:\n",
    "            inverted_index[word].append((doc[\"title\"], doc[\"tf_idf\"][i]))\n",
    "    \n",
    "\n",
    "# print the inverted index for a random word\n",
    "print(inverted_index[\"pandemic\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e07943",
   "metadata": {},
   "source": [
    "Creating a search function using Inverted index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad2c74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the query\n",
    "def tokenize_query(query):\n",
    "    \"\"\"\n",
    "    Preprocesses and tokenizes the text using the specified spaCy model.\n",
    "    Steps:\n",
    "    - Lowercase the text\n",
    "    - Lemmatize\n",
    "    - Remove stopwords, punctuation, and tokens without a proper lemma\n",
    "    \"\"\"\n",
    "    doc = spacy_model(query.lower())\n",
    "    return [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.lemma_ != \"\" and token.lemma_ != \"-PRON-\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ed2fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_inverted_index(query, index = inverted_index):\n",
    "    \"\"\"\n",
    "    Tokenize the query\n",
    "    \"\"\"\n",
    "    tokens = tokenize_query(query)\n",
    "\n",
    "    # create a list which contains the inverted indexes for each token \n",
    "    newlist = []\n",
    "    for token in tokens:\n",
    "        newlist.extend(index[token]) # this will create a list of tuples (title, tf_idf) for each token\n",
    "    \n",
    "    # finally, create a dictionary which contains the results as one document/article can contain multiple tokens\n",
    "    # aggregate the tf_idf values for each title \n",
    "    article_scores = defaultdict(int)\n",
    "    for title, tf_idf in newlist:\n",
    "        article_scores[title] += tf_idf\n",
    "    \n",
    "    # convert the dictionary to a list of tuples\n",
    "    search_results = [(title, score) for title, score in article_scores.items()]\n",
    "\n",
    "    # sort the results based on the tf_idf score in descending order\n",
    "    return sorted(search_results, key=lambda item:item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb0b4d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pandemic prevention', 0.2273367716764853),\n",
       " ('Pandemic Severity Assessment Framework', 0.048804330820589985),\n",
       " ('Event 201', 0.04582303339979459),\n",
       " ('Crimson Contagion', 0.03559722922973019),\n",
       " ('Pandemic', 0.03379752679269161),\n",
       " ('Pandemic severity index', 0.027411818965501027),\n",
       " ('HIV/AIDS', 0.02387466294335984),\n",
       " ('PREDICT (USAID)', 0.02023253304596504),\n",
       " ('Science diplomacy and pandemics', 0.0201193644974329),\n",
       " ('Spanish flu', 0.019024620625310414),\n",
       " ('Disease X', 0.017456393676112226),\n",
       " ('HIV/AIDS in Yunnan', 0.016419317341242027),\n",
       " ('Plague of Cyprian', 0.015736414591306144),\n",
       " ('Swine influenza', 0.014502574871371559),\n",
       " ('COVID-19 pandemic', 0.009881004510820137),\n",
       " ('Antonine Plague', 0.006486766320080395),\n",
       " ('1929â€“1930 psittacosis pandemic', 0.006012498027810367),\n",
       " ('Epidemiology of HIV/AIDS', 0.0029201594086959858),\n",
       " ('Unified Victim Identification System', 0.0028515650601695698),\n",
       " ('Cholera', 0.0017927560926804468)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the search\n",
    "search_inverted_index(\"pandemic prevention organizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "077321fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example_queries.json\", \"r\") as outfile:\n",
    "    example_queries = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8baaaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for query in example_queries:\n",
    "    relevant_article_titles = search_inverted_index(query)\n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"relevant_article_titles\": relevant_article_titles\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5836fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example_queries_results.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
