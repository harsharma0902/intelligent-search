{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee373990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aba346",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16079cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"updated_data.json\", \"r\") as outfile:\n",
    "    updated_data = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all tokenized text into a single string \n",
    "tokenized_texts = [i[\"tokenized_text\"] for i in updated_data]\n",
    "\n",
    "# make tokenized texts into a 1D list\n",
    "corpus_vocab = list(itertools.chain(*tokenized_texts))\n",
    "\n",
    "# remove duplicates from corpus_vocab\n",
    "corpus_vocab = list(set(corpus_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06346a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corpus_vocab to a file\n",
    "with open(\"corpus_vocab.json\", \"w\") as outfile:\n",
    "    json.dump(corpus_vocab, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249cdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of each token in a document (TF)\n",
    "token_counts =  [] # a list of counter objects\n",
    "for documents in updated_data:\n",
    "    doc_tokens = documents[\"tokenized_text\"]\n",
    "    token_count = Counter(doc_tokens)\n",
    "    token_counts.append(token_count)\n",
    "\n",
    "print(token_counts[0])\n",
    "print(len(token_counts[0])) # prints total number of tokens in the document not their frequency\n",
    "\n",
    "# print the number of documents\n",
    "print(len(token_counts)) \n",
    "\n",
    "print(token_counts[0][\"pandemic\"]) # prints the frequency of the token \"pandemic\" in the first document\n",
    "\n",
    "# print the number of tokens in the first document\n",
    "print(len(updated_data[0][\"tokenized_text\"]))\n",
    "print(updated_data[0][\"tokenized_text\"])\n",
    "print(len(updated_data))\n",
    "\n",
    "# counter object is created for each document; list of counter objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each token in the corpus_vocab, check how many documents it appears in \n",
    "num_docs_with_token = {} # a dictionary\n",
    "for token in corpus_vocab:\n",
    "    num_docs = sum([1 for doc in token_counts if token in doc.keys()])\n",
    "    num_docs_with_token[token] = num_docs\n",
    "\n",
    "print(num_docs_with_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_with_token[\"pandemic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be8cdf",
   "metadata": {},
   "source": [
    "Computing TF-IDF Vectors finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(token_counts):\n",
    "    documents_len = len(doc)\n",
    "    tfidf_vector = []\n",
    "\n",
    "    for token in corpus_vocab:\n",
    "        # calculate the term frequency (TF)\n",
    "        tf = doc[token]/len(updated_data[i][\"tokenized_text\"])\n",
    "\n",
    "        # calculate the inverse document frequency (IDF)\n",
    "        idf = np.log(len(updated_data) / num_docs_with_token[token])\n",
    "\n",
    "        tfidf = tf * idf\n",
    "        tfidf_vector.append(tfidf)\n",
    "    \n",
    "    # save the tfidf_vector to the document\n",
    "    updated_data[i][\"tf_idf\"] = tfidf_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"updated_data_vectorized.json\", \"w\") as outfile:\n",
    "    json.dump(updated_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efe356",
   "metadata": {},
   "source": [
    "Create a search function to compute cosine similarities between the document TF-IDF vectors and the query TF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"highest pandemic casualities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbfd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the query\n",
    "def tokenize_query(query):\n",
    "    \"\"\"\n",
    "    Preprocesses and tokenizes the text using the specified spaCy model.\n",
    "    Steps:\n",
    "    - Lowercase the text\n",
    "    - Lemmatize\n",
    "    - Remove stopwords, punctuation, and tokens without a proper lemma\n",
    "    \"\"\"\n",
    "    doc = spacy_model(query.lower())\n",
    "    return [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.lemma_ != \"\" and token.lemma_ != \"-PRON-\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the query just like we developed tf-idf vectors for the documents\n",
    "def vectorize_query(query, corpus_vocab = corpus_vocab):\n",
    "    \"\"\"\n",
    "    Vectorizes the query using the same method as the documents.\n",
    "    \"\"\"\n",
    "    tokenized_query = tokenize_query(query)\n",
    "    query_token_counter = Counter(tokenized_query)\n",
    "    query_vector = []\n",
    "    for token in corpus_vocab:\n",
    "        # calculate TF\n",
    "        tf = query_token_counter[token] / len(tokenized_query)\n",
    "\n",
    "        # calculate IDF\n",
    "        idf = np.log(len(updated_data) / num_docs_with_token[token])\n",
    "        tfidf = tf * idf\n",
    "        \n",
    "        query_vector.append(tfidf)\n",
    "\n",
    "    return query_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize_query(query))\n",
    "print(\"separate\")\n",
    "print(vectorize_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1b260",
   "metadata": {},
   "source": [
    "Finally searching the documents with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf91063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a search function for queries in the documents\n",
    "def search_documents_tfidf(query, docs):\n",
    "\n",
    "    # vectorize the query\n",
    "    query_vector = vectorize_query(query)\n",
    "    query_arr = np.array(query_vector) # convert TF-IDF vector to a numpy array; needed for cosine similarity mathematical operations\n",
    "\n",
    "    # build a list of results and their cosine similarity scores\n",
    "    ranked_results = []\n",
    "    for document in docs:\n",
    "        document_rank = {}\n",
    "        document_array = np.array(document[\"tf_idf\"]) \n",
    "        # calculate cosine similarity: reshape ensures both arrays are 2D; [0][0] gets the similarity score from the resulting 2D array\n",
    "        rank = cosine_similarity(query_arr.reshape(1, -1), document_array.reshape(1, -1))[0][0]\n",
    "\n",
    "        # add the rank to the document based on the condition\n",
    "        if rank > 0: \n",
    "            document_rank[\"title\"] = document[\"title\"]\n",
    "            document_rank[\"rank\"] = rank\n",
    "            ranked_results.append(document_rank)\n",
    "        \n",
    "    # sort the results by rank and return \n",
    "    results = sorted(ranked_results, key=lambda x: x[\"rank\"], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"updated_data_vectorized.json\", \"r\") as outfile:\n",
    "#     updated_data_vectorized = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute search\n",
    "search_documents_tfidf(\"ebola\", updated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
