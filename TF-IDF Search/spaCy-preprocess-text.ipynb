{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e5f2a0",
   "metadata": {},
   "source": [
    "Code to preprocess the text field in data.json and generate tokenized text after lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy numpy scikit-learn ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spacy models (small, medium, large)\n",
    "import spacy\n",
    "import spacy.cli\n",
    "import spacy.cli.download\n",
    "\n",
    "# Download the models if not already present\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "\n",
    "# Load the models\n",
    "nlp_small = spacy.load(\"en_core_web_sm\")\n",
    "nlp_medium = spacy.load(\"en_core_web_md\")\n",
    "nlp_large = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b35972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model sizes in bytes\n",
    "print(f\"Small model size: {nlp_small.vocab.vectors.shape}\")\n",
    "print(f\"Medium model size: {nlp_medium.vocab.vectors.shape}\")\n",
    "print(f\"Large model size: {nlp_large.vocab.vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file and store data in a variable\n",
    "import json\n",
    "\n",
    "with open(\"data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models into a dictionary for easy access\n",
    "models = {\n",
    "    \"small\": nlp_small,\n",
    "    \"medium\": nlp_medium,\n",
    "    \"large\": nlp_large\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text in data.json\n",
    "def tokenize_text(text, model):\n",
    "    \"\"\"\n",
    "    Preprocesses and tokenizes the text using the specified spaCy model.\n",
    "    Steps:\n",
    "    - Lowercase the text\n",
    "    - Lemmatize\n",
    "    - Remove stopwords, punctuation, and tokens without a proper lemma\n",
    "    \"\"\"\n",
    "    nlp = models[model]\n",
    "    doc = nlp(text.lower())\n",
    "    return [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.lemma_ != \"\" and token.lemma_ != \"-PRON-\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized text from small model\n",
    "for entry in data:\n",
    "    entry[\"tokenized_text\"] = tokenize_text(entry[\"text\"], \"small\")\n",
    "\n",
    "# Export the updated data to a new JSON file\n",
    "with open(\"updated_data_small.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenization complete. Updated data saved to updated_data_small.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f80c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized text using the medium model\n",
    "for entry in data:\n",
    "    entry[\"tokenized_text\"] = tokenize_text(entry[\"text\"], \"medium\")\n",
    "\n",
    "# Export the updated data to a new JSON file\n",
    "with open(\"updated_data_medium.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenization complete. Updated data saved to updated_data_medium.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized text from the large model\n",
    "for entry in data:\n",
    "    entry[\"tokenized_text\"] = tokenize_text(entry[\"text\"], \"large\")\n",
    "\n",
    "# Export the updated data to a new JSON file\n",
    "with open(\"updated_data_large.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenization complete. Updated data saved to updated_data_large.json.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
